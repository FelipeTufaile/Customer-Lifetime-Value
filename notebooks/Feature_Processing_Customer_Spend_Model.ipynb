{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FelipeTufaile/customer_lifetime_value/blob/main/notebooks/Feature_Processing_Customer_Spend_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsOcHa0Slj9J"
      },
      "source": [
        "## Installing Kaggle API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YAM8BySSljtB",
        "outputId": "00718100-4a2a-4c7f-ad1e-df999b19aa53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.4)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n"
          ]
        }
      ],
      "source": [
        "# Installing Kaggle API\n",
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yy4uRUwRlwYw"
      },
      "source": [
        "## Configuring Kaggle Credentials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnlj-vyAlU9t"
      },
      "source": [
        "Setup kaggle API correctly following https://www.kaggle.com/docs/api\n",
        "```\n",
        "%%shell\n",
        "mkdir ~/.kaggle\n",
        "echo \\{\\\"username\\\":\\\"{your kaggle username}\\\",\\\"key\\\":\\\"{your kaggle api key}\\\"\\} > ~/.kaggle/kaggle.json\n",
        "pip install kaggle\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "A6whITUBl1DL",
        "outputId": "85c02cd9-82ed-4186-8bea-46e35c3363f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.14)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.4)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "%%shell\n",
        "mkdir ~/.kaggle\n",
        "echo \\{\\\"username\\\":\\\"{your kaggle username}\\\",\\\"key\\\":\\\"{your kaggle api key}\\\"\\} > ~/.kaggle/kaggle.json\n",
        "pip install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBsgg36gp_F8"
      },
      "source": [
        "## Installing PySpark\n",
        "\n",
        "Since part of the feature engineering process may involve dealing with large amounts of data, we will be using PySpark.\n",
        "To facilitate future work, the final processed table will be stored as a parquet file in Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747
        },
        "collapsed": true,
        "id": "TJYSevZzp8Yu",
        "outputId": "9b2c4c58-2963-46b0-fa36-86ac1fd92f18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.81)] [1 InRelease 5,484 B/129\u001b[0m\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.81)] [1 InRelease 46.0 kB/129\u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.81)] [1 InRelease 83.7 kB/129\u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.81)] [Connected to r2u.stat.i\u001b[0m\r                                                                               \rGet:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Connecting to ppa.launchpadcont\u001b[0m\r                                                                               \rIgn:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Connected to ppa.launchpadcontent.net (185.125.190.80\u001b[0m\r                                                                               \rHit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Connected to ppa.launchpadconte\u001b[0m\r                                                                               \rGet:6 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,132 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,119 kB]\n",
            "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [910 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:14 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,218 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,552 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,422 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,396 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [51.8 kB]\n",
            "Fetched 19.1 MB in 2s (8,822 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "49 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488490 sha256=65ec3cbe4e9406dd7e043ea5b9254d239cf447faf04bc290bde128f6f7f346d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/usr/local/lib/python3.10/dist-packages/pyspark'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG2a5JhMRwpd"
      },
      "source": [
        "## Loading Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "sJGlwXgQRznh",
        "outputId": "ac324c2a-e87c-4fb5-9d61-80aff2121942",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7c147384b4f0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://ee143c37da3e:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>DataProcessingPySpark</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Importing Numpy library\n",
        "import numpy as np\n",
        "\n",
        "# Importing Pandas library\n",
        "import pandas as pd\n",
        "\n",
        "# Importing datetime library\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# importing the zipfile module\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Importing gzip library\n",
        "import gzip\n",
        "\n",
        "# Importing plotying libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Importing userdata from Google Colab Library\n",
        "from google.colab import userdata, drive\n",
        "\n",
        "# Importing pyspark libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType, TimestampType\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Creating a spark session\n",
        "spark = SparkSession.builder.appName(\"DataProcessingPySpark\").getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive Folder\n",
        "# This step gives us access to data stored in our Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "atPwEcecAIFL",
        "outputId": "f89d5c33-be82-46d6-eca9-7fe19226ed9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jnd96QbXl8X4"
      },
      "source": [
        "## Downloading Customer Spend Model Data\n",
        "\n",
        "This notebook use the customer spend model data available in the Kaggle. The dataset can be found in the following link: https://www.kaggle.com/competitions/customer-spend-model/data.\n",
        "The task requested by the author of the competition is to **predict customer purchases from German book company**. In order to do so, we are provided with the transaction history for each customer up to Nov 24, 2014.\n",
        "The output that we are requested to provide is the log of the estimated amount spent by each customer after Nov 24, 2014. In order to avoid log(0), we are requested to adjust the log calculation to log(x+1), where x is the estimate of the future amount spent.\n",
        "For more details about the task description you can access the following link: https://www.kaggle.com/competitions/customer-spend-model/overview."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykfl_kK2h5I-",
        "outputId": "b63a0104-ea31-4e7b-b8c6-028470ce79ef"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "customer-spend-model.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "# Downloading the Customer Spend Model Data from Kaggle\n",
        "!kaggle competitions download -c customer-spend-model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB-ZPtPporjr"
      },
      "source": [
        "## Unzipping the Transactions Dataset\n",
        "\n",
        "This is step is necessary to unzip the csv file download in the previous cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhIt3xqLmnFG"
      },
      "outputs": [],
      "source": [
        "# loading the customer-spend-model.zip and creating a zip object\n",
        "with ZipFile(\"../content/customer-spend-model.zip\", 'r') as zip_object:\n",
        "\n",
        "\t# Extracting specific file in the zip into a specific location.\n",
        "\tzip_object.extract(\"customer.csv\", path=\"../content/drive/MyDrive/Colab/Sandbox/Data\")\n",
        "\n",
        " \t# Extracting specific file in the zip into a specific location.\n",
        "\tzip_object.extract(\"orders.csv\", path=\"../content/drive/MyDrive/Colab/Sandbox/Data\")\n",
        "\n",
        "# closing object\n",
        "zip_object.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qecZproxKwV"
      },
      "source": [
        "## Processing Data Using PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McLBia8pc5DH"
      },
      "source": [
        "### Loading Orders Dataframe\n",
        "\n",
        "**orders.csv**: all orders prior to 11/25/2014 for training (n=5,551) and test (n=11,230) sets. You should find 353,687 records plus a header.\n",
        "\n",
        "- **id**: unique customer identifier;\n",
        "- **orddate**: order date;\n",
        "- **ordnum**: order number;\n",
        "- **category**:\n",
        "    - category identifier:,\n",
        "        - 1 = fiction;\n",
        "        - 3 = classics;\n",
        "        - 5 = cartoons;\n",
        "        - 6 = legends;\n",
        "        - 7 = philosophy;\n",
        "        - 8 = religion;\n",
        "        - 9 = psychology;\n",
        "        - 10 = linguistics;\n",
        "        - 12 = art;\n",
        "        - 14 = music;\n",
        "        - 17 = art reprints;\n",
        "        - 19 = history;\n",
        "        - 20 = contemporary history;\n",
        "        - 21 = economy;\n",
        "        - 22 = politics;\n",
        "        - 23 = science;\n",
        "        - 26 = computer science;\n",
        "        - 27 = traffic, railroads;\n",
        "        - 30 = maps;\n",
        "        - 31 = travel guides;\n",
        "        - 35 = health;\n",
        "        - 36 = cooking;\n",
        "        - 37 = learning;\n",
        "        - 38 = games and riddles;\n",
        "        - 39 = sports;\n",
        "        - 40 = hobby;\n",
        "        - 41 = nature/animals/plants;\n",
        "        - 44 = encyclopedia;\n",
        "        - 50 = videos, DVDs;\n",
        "        - 99 = non books\n",
        "- **qty**: quantity;\n",
        "- **price**: price paid;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpa6qQkDXeAm",
        "outputId": "9cc47d06-96ad-44ae-d349-5ea85611eee0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+------+--------+---+------------+\n",
            "| id|  orddate|ordnum|category|qty|       price|\n",
            "+---+---------+------+--------+---+------------+\n",
            "|957|10FEB2008| 38650|      35|  1|5.0106582642|\n",
            "|957|10FEB2008| 38650|      35|  1|20.426101685|\n",
            "|957|10FEB2008| 38650|      19|  1|20.400543213|\n",
            "|957|15MAR2008| 48972|      40|  1|25.539016724|\n",
            "|957|22NOV2008|150011|      40|  1|14.316169739|\n",
            "|957|22NOV2008|150011|      40|  1|8.5896987915|\n",
            "|957|03OCT2009|286151|      19|  1|15.313186646|\n",
            "|957|04APR2010|376779|      14|  1|12.782295227|\n",
            "|957|04APR2010|376779|      14|  1|5.0873527527|\n",
            "|957|04APR2010|376779|      35|  1|6.5445327759|\n",
            "|957|14AUG2011|622093|      99|  1|8.6919555664|\n",
            "|957|14AUG2011|622093|      19|  1|10.174705505|\n",
            "|957|14AUG2011|622093|       5|  1|15.236495972|\n",
            "|957|10SEP2011|639810|      99|  1|9.9497375488|\n",
            "|957|10SEP2011|639810|      35|  1|10.200271606|\n",
            "|957|10SEP2011|639810|      99|  1|6.5445327759|\n",
            "|957|10SEP2011|639810|      99|  1|2.5564575195|\n",
            "|957|10SEP2011|639810|      14|  1|6.5445327759|\n",
            "|957|10OCT2011|655931|      35|  1|10.174705505|\n",
            "|957|10OCT2011|655931|      40|  1|12.731163025|\n",
            "+---+---------+------+--------+---+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Reading orders dataframe\n",
        "orders_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../content/drive/MyDrive/Colab/Sandbox/Data/orders.csv\")\n",
        "\n",
        "# Show orders dataframe\n",
        "orders_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PcXHNqac5DI"
      },
      "source": [
        "### Creating a Transaction Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vy7P4jaBZC0p",
        "outputId": "2ac596e0-72a0-4be4-b8e0-cbbbebaa9e83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in the transaction dataframe: 92245\n",
            "+--------+----------+----------+------------------+\n",
            "| cust_id|    ref_dt|  trans_dt|         trans_amt|\n",
            "+--------+----------+----------+------------------+\n",
            "|  100021|2009-02-01|2009-02-25|      6.6212272644|\n",
            "|  100021|2010-01-01|2010-01-20|     33.0805740352|\n",
            "|  100021|2010-03-01|2010-03-06|     45.8117370608|\n",
            "|  100021|2010-04-01|2010-04-14|        50.1065979|\n",
            "|  100021|2010-05-01|2010-05-10|27.047328949399997|\n",
            "|  100021|2010-08-01|2010-08-28|      50.899093628|\n",
            "|  100021|2010-09-01|2010-09-26|      15.287620544|\n",
            "|10005188|2012-07-01|2012-07-28|30.451723098600002|\n",
            "|10005188|2012-08-01|2012-08-26|     45.2999877928|\n",
            "|10005188|2013-05-01|2013-05-08|24.799987792899998|\n",
            "|10005188|2013-09-01|2013-09-22|42.849990844000004|\n",
            "|10005188|2014-06-01|2014-06-15|      19.799987793|\n",
            "|10005188|2014-10-01|2014-10-04|     19.8999938964|\n",
            "|10009396|2012-08-01|2012-08-07|     24.9506454467|\n",
            "|10009396|2013-01-01|2013-01-05|189.09993743780004|\n",
            "|10009396|2013-10-01|2013-10-07|      69.900253295|\n",
            "|10009396|2014-06-01|2014-06-01| 68.49997520360002|\n",
            "|10009396|2014-07-01|2014-07-13| 49.69998168920001|\n",
            "|10009604|2013-05-01|2013-05-04|     14.8999938965|\n",
            "|10009604|2013-08-01|2013-08-31|     29.7499923706|\n",
            "+--------+----------+----------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Creating a transaction dataframe\n",
        "trans_df = (\n",
        "\n",
        "    # Referencing orders dataframe\n",
        "    orders_df\n",
        "\n",
        "    # Converting the transaction date format\n",
        "    .withColumn(\"trans_dt\", f.date_format(f.to_date(f.col(\"orddate\"), \"ddMMMyyyy\"), 'yyyy-MM-dd'))\n",
        "\n",
        "    # Selecting columns\n",
        "    .select(\n",
        "        f.col(\"id\").alias(\"cust_id\"),\n",
        "        f.trunc(\"trans_dt\", \"month\").alias(\"ref_dt\"),\n",
        "        f.col(\"trans_dt\"),\n",
        "        #f.col(\"category\"),                             # Will not be used right now\n",
        "        #f.col(\"qty\").alias(\"trans_qnt\"),               # Will not be used right now\n",
        "        f.col(\"price\").alias(\"trans_amt\")\n",
        "    )\n",
        "\n",
        "    # Consider only purchase amount higher than 0. Purchases amounts <= 0 does not make sense.\n",
        "    .filter(f.col(\"trans_amt\") > 0)\n",
        "\n",
        "    # Agregating by customer id and transaction date\n",
        "    .groupBy([\"cust_id\", \"ref_dt\"])\n",
        "    .agg(\n",
        "        f.min(\"trans_dt\").alias(\"trans_dt\"),\n",
        "        f.sum(\"trans_amt\").alias(\"trans_amt\")\n",
        "    )\n",
        "\n",
        "    # Ordering by customer id and transaction date\n",
        "    .orderBy(f.asc(\"cust_id\"), f.asc(\"trans_dt\"))\n",
        "\n",
        "# The caching step here aims to avoid crashing the feature processing, possibly due to long DAGs.\n",
        ").cache()\n",
        "\n",
        "# Counting the number of rows and caching the dataframe\n",
        "print(f\"Number of rows in the transaction dataframe: {trans_df.count()}\")\n",
        "\n",
        "# Printing the transaction dataframe\n",
        "trans_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4uKfe14Iw1n"
      },
      "source": [
        "## Creating a Customer Lifetime value Dataframe\n",
        "\n",
        "Next we will create a customer lifetime value dataframe. This dataframe will be a representation \"snapshot\" of each customer's behavior pattern on 24 Nov 2014, which considers each customers transactions history. Historically, features that best describe customer behavior are RFML features (Recency, Frequency, Monetary and Lifetime). Therefore, we will calculate RFML features along with additional features (e.g. Rolling 6 months transaction amounts)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znKZhpcbJJjt",
        "outputId": "ad96a90f-9014-4be0-cfdf-fe7804d4c003"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First transaction date across all customers: 2007-11-01\n"
          ]
        }
      ],
      "source": [
        "# Calculating the first transaction date accross all customers\n",
        "start_date = trans_df.agg({\"trans_dt\": \"min\"}).withColumnRenamed(\"min(trans_dt)\", \"date\").collect()[0].date[:-2]+'01'\n",
        "\n",
        "print(f\"First transaction date across all customers: {start_date}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXhJC0f4KXVA",
        "outputId": "9634b273-7525-45bd-eb9f-699e8eba955f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last transaction date across all customers: 2014-11-01\n"
          ]
        }
      ],
      "source": [
        "# Calculating the last transaction date accross all customers\n",
        "end_date = trans_df.agg({\"trans_dt\": \"max\"}).withColumnRenamed(\"max(trans_dt)\", \"date\").collect()[0].date[:-2]+'01'\n",
        "\n",
        "print(f\"Last transaction date across all customers: {end_date}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although we have a specific task of generating predictions for purchases after 24 Nov 2014, we can actually train our model on much broader timeframe. That is, we can generate a dataframe containing information on customers behavior for each month/year available for each customer instead of using only the \"snapshot\" on 24 Nov 2014.\n",
        "\n",
        "The next feature processing step will generate such dataframe containing information on customer behavior for each valid combination of month/year and customer id."
      ],
      "metadata": {
        "id": "wt7F7KZDi9cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a PySpark calendar dataframe with month-start (MS) frequency\n",
        "calendar_df = (\n",
        "  spark.createDataFrame(pd.DataFrame({'ref_dt': pd.date_range(start=start_date, end=end_date, freq='MS')}))\n",
        "  .withColumn(\"ref_dt\", f.to_date(\"ref_dt\"))\n",
        ").cache()\n",
        "\n",
        "# Counting the number of rows and caching the dataframe\n",
        "print(f\"Number of rows in the calendar dataframe: {calendar_df.count()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfEjViaXi1Ax",
        "outputId": "cde0c111-bce7-4c5f-a523-276fe7ba4669"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in the calendar dataframe: 85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a customer id dataframe\n",
        "# This dataframe should contain all distinct customer ids available in the transaction dataframe\n",
        "customers_id_df = trans_df.select(f.col('cust_id')).distinct().cache()\n",
        "\n",
        "# Counting the number of rows and caching the dataframe\n",
        "print(f\"Number of rows in the customer id dataframe: {customers_id_df.count()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ywcnq_loi0UN",
        "outputId": "52d98422-a443-4748-d95e-b21c7285209a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in the customer id dataframe: 16661\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a cross-join dataframe\n",
        "cross_join_df = (\n",
        "\n",
        "    # Referencing the calendar table\n",
        "    calendar_df\n",
        "\n",
        "    # Performing a crossjoin with distinct customer ids and distinct transaction dates.\n",
        "    # This should result in one record per month per customer\n",
        "    .crossJoin(customers_id_df)\n",
        "\n",
        "    # Adding aggregated transaction information about customers\n",
        "    .join(trans_df, on=[\"cust_id\", \"ref_dt\"], how=\"left\")\n",
        "\n",
        "    # Calculating the first transaction date per customer\n",
        "    .withColumn(\"first_trans_dt\", f.min(\"trans_dt\").over(Window.partitionBy([\"cust_id\"])))\n",
        "\n",
        "    # Selecting only the dates after the first transaction date for each customer\n",
        "    .filter(f.col(\"ref_dt\") >= f.trunc(\"first_trans_dt\", \"month\"))\n",
        "\n",
        "    # Replace null transaction amounts to zero\n",
        "    .withColumn(\"trans_amt\", f.when(f.col(\"trans_amt\").isNull(), f.lit(0)).otherwise(f.col(\"trans_amt\")))\n",
        "\n",
        "    # Calculating the last transaction date per customer\n",
        "    .withColumn(\"last_trans_dt\", f.last(\"trans_dt\", ignorenulls=True).over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(Window.unboundedPreceding, 0)))\n",
        "\n",
        "    # Ordering columns\n",
        "    .select(\n",
        "        \"cust_id\",\n",
        "        \"ref_dt\",\n",
        "        \"trans_amt\",\n",
        "        \"first_trans_dt\",\n",
        "        \"last_trans_dt\"\n",
        "    )\n",
        "\n",
        ").cache()\n",
        "\n",
        "\n",
        "# Counting the number of rows and caching the dataframe\n",
        "print(f\"Number of rows in the cross-join dataframe: {cross_join_df.count()}\")\n",
        "\n",
        "# Printing cross-join dataframe\n",
        "cross_join_df.show()"
      ],
      "metadata": {
        "id": "C6b8wPRWPcQI",
        "outputId": "fb59cc57-b863-4787-caec-2d70b833bc34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in the cross-join dataframe: 749989\n",
            "+--------+----------+-------------+--------------+-------------+\n",
            "| cust_id|    ref_dt|    trans_amt|first_trans_dt|last_trans_dt|\n",
            "+--------+----------+-------------+--------------+-------------+\n",
            "|10094474|2013-12-01| 30.799732208|    2013-12-02|   2013-12-02|\n",
            "|10094474|2014-01-01|          0.0|    2013-12-02|   2013-12-02|\n",
            "|10094474|2014-02-01|          0.0|    2013-12-02|   2013-12-02|\n",
            "|10094474|2014-03-01|          0.0|    2013-12-02|   2013-12-02|\n",
            "|10094474|2014-04-01|          0.0|    2013-12-02|   2013-12-02|\n",
            "|10094474|2014-05-01| 25.849992752|    2013-12-02|   2014-05-04|\n",
            "|10094474|2014-06-01|36.9399871822|    2013-12-02|   2014-06-18|\n",
            "|10094474|2014-07-01|          0.0|    2013-12-02|   2014-06-18|\n",
            "|10094474|2014-08-01|          0.0|    2013-12-02|   2014-06-18|\n",
            "|10094474|2014-09-01|          0.0|    2013-12-02|   2014-06-18|\n",
            "|10094474|2014-10-01| 29.989990234|    2013-12-02|   2014-10-07|\n",
            "|10094474|2014-11-01|          0.0|    2013-12-02|   2014-10-07|\n",
            "|10425071|2013-04-01|54.3499908444|    2013-04-27|   2013-04-27|\n",
            "|10425071|2013-05-01|          0.0|    2013-04-27|   2013-04-27|\n",
            "|10425071|2013-06-01|          0.0|    2013-04-27|   2013-04-27|\n",
            "|10425071|2013-07-01|          0.0|    2013-04-27|   2013-04-27|\n",
            "|10425071|2013-08-01|          0.0|    2013-04-27|   2013-04-27|\n",
            "|10425071|2013-09-01|          0.0|    2013-04-27|   2013-04-27|\n",
            "|10425071|2013-10-01|          0.0|    2013-04-27|   2013-04-27|\n",
            "|10425071|2013-11-01|          0.0|    2013-04-27|   2013-04-27|\n",
            "+--------+----------+-------------+--------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VDwVHUR6XaD",
        "outputId": "a72df2a0-6ed4-492a-c10c-1d8dbd20bb96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dataframe has: 749989 rows\n"
          ]
        }
      ],
      "source": [
        "# Creating a customer lifetime value dataframe\n",
        "cltv_df = (\n",
        "\n",
        "    # Referencing the cross-join table\n",
        "    cross_join_df\n",
        "\n",
        "    # Get month number from reference date\n",
        "    .withColumn(\"ref_month\", f.month(\"ref_dt\"))\n",
        "\n",
        "    # Get month number from reference date\n",
        "    .withColumn(\"ref_year\", f.year(\"ref_dt\"))\n",
        "\n",
        "    # Calculating first transaction amount per customer\n",
        "    .withColumn(\"first_trans_amt\", f.first(\"trans_amt\", ignorenulls=True).over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(Window.unboundedPreceding, 0)))\n",
        "\n",
        "    # Creating a transaction indicator\n",
        "    .withColumn(\"trans_ind\", f.when(f.col(\"trans_amt\") > 0, f.lit(1)).otherwise(f.lit(0)))\n",
        "\n",
        "    # Calculating the recency of each customer: the number of months between the reference date and the last transaction date\n",
        "    .withColumn(\"recency\", f.datediff(f.col(\"ref_dt\"), f.col(\"last_trans_dt\"))/(365/12))\n",
        "\n",
        "    # Calculating Frequency: The number of months with transactions per customer between the firt transaction date and the reference date\n",
        "    .withColumn(\"frequency\", f.sum(\"trans_ind\").over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(Window.unboundedPreceding, 0)))\n",
        "\n",
        "    # Calculating Monetary: The total amount spent per customer between the firt transaction date and the reference date\n",
        "    .withColumn(\"monetary\", f.sum(\"trans_amt\").over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(Window.unboundedPreceding, 0)))\n",
        "\n",
        "    # Calculating the lifetime of each customer: the number of months between the reference date and the first transaction date\n",
        "    .withColumn(\"lifetime\", f.datediff(f.col(\"ref_dt\"), f.col(\"first_trans_dt\"))/(365/12))\n",
        "\n",
        "    # Calculating cycle length: the average number of months between subsequent transaction\n",
        "    .withColumn(\"cycle_length\", f.col(\"lifetime\") / f.col(\"frequency\"))\n",
        "\n",
        "    # Calculating rolling 3 months transaction quantity\n",
        "    .withColumn(\"trans_qnt_R03m_lag0\", f.sum(\"trans_ind\").over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(-2, 0)))\n",
        "    .withColumn(\"trans_qnt_R03m_lag1\", f.sum(\"trans_ind\").over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(-5, -3)))\n",
        "    .withColumn(\"trans_qnt_R03m_lag2\", f.sum(\"trans_ind\").over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(-8, -6)))\n",
        "    .withColumn(\"trans_qnt_R03m_lag3\", f.sum(\"trans_ind\").over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(-11, -9)))\n",
        "\n",
        "    # Calculating rolling 3 months transaction amount\n",
        "    .withColumn(\"trans_amt_R03m_lag0\", f.sum(\"trans_amt\").over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(-2, 0)))\n",
        "    .withColumn(\"trans_amt_R03m_lag1\", f.sum(\"trans_amt\").over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(-5, -3)))\n",
        "    .withColumn(\"trans_amt_R03m_lag2\", f.sum(\"trans_amt\").over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(-8, -6)))\n",
        "    .withColumn(\"trans_amt_R03m_lag3\", f.sum(\"trans_amt\").over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(-11, -9)))\n",
        "\n",
        "    # Calculating the transaction amount for next 3 months\n",
        "    # This column can be used as a target column (as an alternative for logtarg) for model training\n",
        "    .withColumn(\"trans_amt_R03m_lead1\",  f.sum(\"trans_amt\").over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(1, 3)))\n",
        "\n",
        "    # Fill missing values with 0\n",
        "    # This is step will add 0 to the rolling 3 months calculations in cases where there is not enough transaction history to calculate the corresponding feature\n",
        "    .fillna(0)\n",
        "\n",
        "    # Ordering and selecting columns\n",
        "    .select(\n",
        "      \"cust_id\",\n",
        "      \"ref_dt\",\n",
        "      \"ref_year\",\n",
        "      \"ref_month\",\n",
        "      \"first_trans_amt\",\n",
        "      \"cycle_length\",\n",
        "      \"recency\",\n",
        "      \"frequency\",\n",
        "      \"monetary\",\n",
        "      \"lifetime\",\n",
        "      \"trans_qnt_R03m_lag0\",\n",
        "      \"trans_qnt_R03m_lag1\",\n",
        "      \"trans_qnt_R03m_lag2\",\n",
        "      \"trans_qnt_R03m_lag3\",\n",
        "      \"trans_amt_R03m_lag0\",\n",
        "      \"trans_amt_R03m_lag1\",\n",
        "      \"trans_amt_R03m_lag2\",\n",
        "      \"trans_amt_R03m_lag3\",\n",
        "      \"trans_amt_R03m_lead1\", # This feature can be used as target\n",
        "    )\n",
        "\n",
        "    # Ordering by customer id and transaction date\n",
        "    .orderBy(f.asc(\"cust_id\"), f.asc(\"ref_dt\"))\n",
        "\n",
        ").cache()\n",
        "\n",
        "print(f\"The dataframe has: {cltv_df.count()} rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfpkJeHqBBH_",
        "outputId": "4093136d-128b-4659-b355-8323460898b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+--------+---------+---------------+-------------------+--------------------+---------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------------+\n",
            "|cust_id|    ref_dt|ref_year|ref_month|first_trans_amt|       cycle_length|             recency|frequency|          monetary|           lifetime|trans_qnt_R03m_lag0|trans_qnt_R03m_lag1|trans_qnt_R03m_lag2|trans_qnt_R03m_lag3|trans_amt_R03m_lag0|trans_amt_R03m_lag1|trans_amt_R03m_lag2|trans_amt_R03m_lag3|trans_amt_R03m_lead1|\n",
            "+-------+----------+--------+---------+---------------+-------------------+--------------------+---------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------------+\n",
            "| 100021|2009-02-01|    2009|        2|   6.6212272644|-0.7890410958904109| -0.7890410958904109|        1|      6.6212272644|-0.7890410958904109|                  1|                  0|                  0|                  0|       6.6212272644|                0.0|                0.0|                0.0|                 0.0|\n",
            "| 100021|2009-03-01|    2009|        3|   6.6212272644|0.13150684931506848| 0.13150684931506848|        1|      6.6212272644|0.13150684931506848|                  1|                  0|                  0|                  0|       6.6212272644|                0.0|                0.0|                0.0|                 0.0|\n",
            "| 100021|2009-04-01|    2009|        4|   6.6212272644| 1.1506849315068493|  1.1506849315068493|        1|      6.6212272644| 1.1506849315068493|                  1|                  0|                  0|                  0|       6.6212272644|                0.0|                0.0|                0.0|                 0.0|\n",
            "| 100021|2009-05-01|    2009|        5|   6.6212272644|  2.136986301369863|   2.136986301369863|        1|      6.6212272644|  2.136986301369863|                  0|                  1|                  0|                  0|                0.0|       6.6212272644|                0.0|                0.0|                 0.0|\n",
            "| 100021|2009-06-01|    2009|        6|   6.6212272644| 3.1561643835616437|  3.1561643835616437|        1|      6.6212272644| 3.1561643835616437|                  0|                  1|                  0|                  0|                0.0|       6.6212272644|                0.0|                0.0|                 0.0|\n",
            "| 100021|2009-07-01|    2009|        7|   6.6212272644|  4.142465753424657|   4.142465753424657|        1|      6.6212272644|  4.142465753424657|                  0|                  1|                  0|                  0|                0.0|       6.6212272644|                0.0|                0.0|                 0.0|\n",
            "| 100021|2009-08-01|    2009|        8|   6.6212272644|  5.161643835616438|   5.161643835616438|        1|      6.6212272644|  5.161643835616438|                  0|                  0|                  1|                  0|                0.0|                0.0|       6.6212272644|                0.0|                 0.0|\n",
            "| 100021|2009-09-01|    2009|        9|   6.6212272644|  6.180821917808219|   6.180821917808219|        1|      6.6212272644|  6.180821917808219|                  0|                  0|                  1|                  0|                0.0|                0.0|       6.6212272644|                0.0|                 0.0|\n",
            "| 100021|2009-10-01|    2009|       10|   6.6212272644|  7.167123287671233|   7.167123287671233|        1|      6.6212272644|  7.167123287671233|                  0|                  0|                  1|                  0|                0.0|                0.0|       6.6212272644|                0.0|       33.0805740352|\n",
            "| 100021|2009-11-01|    2009|       11|   6.6212272644|  8.186301369863013|   8.186301369863013|        1|      6.6212272644|  8.186301369863013|                  0|                  0|                  0|                  1|                0.0|                0.0|                0.0|       6.6212272644|       33.0805740352|\n",
            "| 100021|2009-12-01|    2009|       12|   6.6212272644|  9.172602739726027|   9.172602739726027|        1|      6.6212272644|  9.172602739726027|                  0|                  0|                  0|                  1|                0.0|                0.0|                0.0|       6.6212272644|        78.892311096|\n",
            "| 100021|2010-01-01|    2010|        1|   6.6212272644|  5.095890410958904| -0.6246575342465753|        2|     39.7018012996| 10.191780821917808|                  1|                  0|                  0|                  1|      33.0805740352|                0.0|                0.0|       6.6212272644|       95.9183349608|\n",
            "| 100021|2010-02-01|    2010|        2|   6.6212272644|  5.605479452054794| 0.39452054794520547|        2|     39.7018012996| 11.210958904109589|                  1|                  0|                  0|                  0|      33.0805740352|                0.0|                0.0|                0.0|  122.96566391019999|\n",
            "| 100021|2010-03-01|    2010|        3|   6.6212272644|  4.043835616438356| -0.1643835616438356|        3| 85.51353836039999| 12.131506849315068|                  2|                  0|                  0|                  0|       78.892311096|                0.0|                0.0|                0.0|       77.1539268494|\n",
            "| 100021|2010-04-01|    2010|        4|   6.6212272644|  3.287671232876712|-0.42739726027397257|        4|    135.6201362604| 13.150684931506849|                  2|                  1|                  0|                  0|      95.9183349608|      33.0805740352|                0.0|                0.0|  27.047328949399997|\n",
            "| 100021|2010-05-01|    2010|        5|   6.6212272644| 2.8273972602739725| -0.2958904109589041|        5|162.66746520979999| 14.136986301369863|                  3|                  1|                  0|                  0| 122.96566391019999|      33.0805740352|                0.0|                0.0|        50.899093628|\n",
            "| 100021|2010-06-01|    2010|        6|   6.6212272644| 3.0312328767123287|  0.7232876712328767|        5|162.66746520979999| 15.156164383561643|                  2|                  2|                  0|                  0|      77.1539268494|       78.892311096|                0.0|                0.0|        66.186714172|\n",
            "| 100021|2010-07-01|    2010|        7|   6.6212272644|  3.228493150684931|  1.7095890410958903|        5|162.66746520979999| 16.142465753424656|                  1|                  2|                  1|                  0| 27.047328949399997|      95.9183349608|      33.0805740352|                0.0|        66.186714172|\n",
            "| 100021|2010-08-01|    2010|        8|   6.6212272644| 2.8602739726027395| -0.8876712328767123|        6|    213.5665588378| 17.161643835616438|                  1|                  3|                  1|                  0|       50.899093628| 122.96566391019999|      33.0805740352|                0.0|        15.287620544|\n",
            "| 100021|2010-09-01|    2010|        9|   6.6212272644| 2.5972602739726023|  -0.821917808219178|        7|228.85417938179998| 18.180821917808217|                  2|                  2|                  2|                  0|       66.186714172|      77.1539268494|       78.892311096|                0.0|                 0.0|\n",
            "+-------+----------+--------+---------+---------------+-------------------+--------------------+---------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Printing a sample of the dataset\n",
        "cltv_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5p6HDaA0YZJ"
      },
      "source": [
        "## Storing the Processed Dataset as Parquet Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Nwa59wh9yHj-"
      },
      "outputs": [],
      "source": [
        "# Defining dataframe path\n",
        "file_path = \"../content/drive/MyDrive/Colab/Sandbox/customer_spend_model\"\n",
        "\n",
        "# Storing Customer Lifetime Value Data\n",
        "cltv_df.write.format(\"parquet\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(file_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}