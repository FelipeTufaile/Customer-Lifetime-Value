{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FelipeTufaile/customer_lifetime_value/blob/main/notebooks/Customer_Spend_Model_Feature_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsOcHa0Slj9J"
      },
      "source": [
        "## Installing Kaggle API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YAM8BySSljtB",
        "outputId": "65841da7-ad43-4253-b07d-d11d9cdda3a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.14)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.4)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n"
          ]
        }
      ],
      "source": [
        "# Installing Kaggle API\n",
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yy4uRUwRlwYw"
      },
      "source": [
        "## Configuring Kaggle Credentials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnlj-vyAlU9t"
      },
      "source": [
        "Setup kaggle API correctly following https://www.kaggle.com/docs/api\n",
        "```\n",
        "%%shell\n",
        "mkdir ~/.kaggle\n",
        "echo \\{\\\"username\\\":\\\"{your kaggle username}\\\",\\\"key\\\":\\\"{your kaggle api key}\\\"\\} > ~/.kaggle/kaggle.json\n",
        "pip install kaggle\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "A6whITUBl1DL",
        "outputId": "c633244d-215b-4e16-df82-9e75d5b71182"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.14)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.4)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "%%shell\n",
        "mkdir ~/.kaggle\n",
        "echo \\{\\\"username\\\":\\\"{your kaggle username}\\\",\\\"key\\\":\\\"{your kaggle api key}\\\"\\} > ~/.kaggle/kaggle.json\n",
        "pip install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBsgg36gp_F8"
      },
      "source": [
        "## Installing PySpark\n",
        "\n",
        "Since part of the feature engineering process may involve dealing with large amounts of data, we will be using PySpark.\n",
        "To facilitate future work, the final processed table will be stored as a parquet file in Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        },
        "collapsed": true,
        "id": "TJYSevZzp8Yu",
        "outputId": "4c99ef2c-062e-409b-e2bd-1fa18a7e6050"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Ign:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,129 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,206 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,378 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,549 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,420 kB]\n",
            "Fetched 16.1 MB in 7s (2,467 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "47 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488490 sha256=69ada6ab641dc37f8c6cae5be5db935ab179d72bf90d91f19bad25b95d194b45\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/usr/local/lib/python3.10/dist-packages/pyspark'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG2a5JhMRwpd"
      },
      "source": [
        "## Loading Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "sJGlwXgQRznh",
        "outputId": "319483d7-73b5-4d08-e087-1dcadd6fb2e9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f5598eb6a70>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://35e658234644:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>DataProcessingPySpark</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Importing Numpy library\n",
        "import numpy as np\n",
        "\n",
        "# Importing Pandas library\n",
        "import pandas as pd\n",
        "\n",
        "# Importing datetime library\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# importing the zipfile module\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Importing gzip library\n",
        "import gzip\n",
        "\n",
        "# Importing plotying libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Importing userdata from Google Colab Library\n",
        "from google.colab import userdata, drive\n",
        "\n",
        "# Importing pyspark libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType, TimestampType\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Creating a spark session\n",
        "spark = SparkSession.builder.appName(\"DataProcessingPySpark\").getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jnd96QbXl8X4"
      },
      "source": [
        "## Downloading Customer Spend Model Data\n",
        "\n",
        "This notebook use the customer spend model data available in the Kaggle. The dataset can be found in the following link: https://www.kaggle.com/competitions/customer-spend-model/data.\n",
        "The task requested by the author of the competition is to **predict customer purchases from German book company**. In order to do so, we are provided with the transaction history for each customer up to Nov 24, 2014.\n",
        "The output that we are requested to provide is the log of the estimated amount spent by each customer after Nov 24, 2014. In order to avoid log(0), we are requested to adjust the log calculation to log(x+1), where x is the estimate of the future amount spent.\n",
        "For more details about the task description you can access the following link: https://www.kaggle.com/competitions/customer-spend-model/overview."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykfl_kK2h5I-",
        "outputId": "2a3ec6a2-62f6-4e78-9166-56b679f42cce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading customer-spend-model.zip to /content\n",
            " 89% 2.00M/2.24M [00:00<00:00, 2.90MB/s]\n",
            "100% 2.24M/2.24M [00:00<00:00, 2.80MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Downloading the Customer Spend Model Data from Kaggle\n",
        "!kaggle competitions download -c customer-spend-model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB-ZPtPporjr"
      },
      "source": [
        "## Unzipping the Transactions Dataset\n",
        "\n",
        "This is step is necessary to unzip the csv file download in the previous cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jhIt3xqLmnFG"
      },
      "outputs": [],
      "source": [
        "# loading the customer-spend-model.zip and creating a zip object\n",
        "with ZipFile(\"../content/customer-spend-model.zip\", 'r') as zip_object:\n",
        "\n",
        "\t# Extracting specific file in the zip into a specific location.\n",
        "\tzip_object.extract(\"customer.csv\", path=\"../content\")\n",
        "\n",
        " \t# Extracting specific file in the zip into a specific location.\n",
        "\tzip_object.extract(\"orders.csv\", path=\"../content\")\n",
        "\n",
        "# closing object\n",
        "zip_object.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qecZproxKwV"
      },
      "source": [
        "## Processing Data Using PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McLBia8pc5DH"
      },
      "source": [
        "### Loading Orders Dataframe\n",
        "\n",
        "**orders.csv**: all orders prior to 11/25/2014 for training (n=5,551) and test (n=11,230) sets. You should find 353,687 records plus a header.\n",
        "\n",
        "- **id**: unique customer identifier;\n",
        "- **orddate**: order date;\n",
        "- **ordnum**: order number;\n",
        "- **category**:\n",
        "    - category identifier:,\n",
        "        - 1 = fiction;\n",
        "        - 3 = classics;\n",
        "        - 5 = cartoons;\n",
        "        - 6 = legends;\n",
        "        - 7 = philosophy;\n",
        "        - 8 = religion;\n",
        "        - 9 = psychology;\n",
        "        - 10 = linguistics;\n",
        "        - 12 = art;\n",
        "        - 14 = music;\n",
        "        - 17 = art reprints;\n",
        "        - 19 = history;\n",
        "        - 20 = contemporary history;\n",
        "        - 21 = economy;\n",
        "        - 22 = politics;\n",
        "        - 23 = science;\n",
        "        - 26 = computer science;\n",
        "        - 27 = traffic, railroads;\n",
        "        - 30 = maps;\n",
        "        - 31 = travel guides;\n",
        "        - 35 = health;\n",
        "        - 36 = cooking;\n",
        "        - 37 = learning;\n",
        "        - 38 = games and riddles;\n",
        "        - 39 = sports;\n",
        "        - 40 = hobby;\n",
        "        - 41 = nature/animals/plants;\n",
        "        - 44 = encyclopedia;\n",
        "        - 50 = videos, DVDs;\n",
        "        - 99 = non books\n",
        "- **qty**: quantity;\n",
        "- **price**: price paid;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpa6qQkDXeAm",
        "outputId": "afc36100-a379-42cc-e447-898d21971aa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+------+--------+---+------------+\n",
            "| id|  orddate|ordnum|category|qty|       price|\n",
            "+---+---------+------+--------+---+------------+\n",
            "|957|10FEB2008| 38650|      35|  1|5.0106582642|\n",
            "|957|10FEB2008| 38650|      35|  1|20.426101685|\n",
            "|957|10FEB2008| 38650|      19|  1|20.400543213|\n",
            "|957|15MAR2008| 48972|      40|  1|25.539016724|\n",
            "|957|22NOV2008|150011|      40|  1|14.316169739|\n",
            "|957|22NOV2008|150011|      40|  1|8.5896987915|\n",
            "|957|03OCT2009|286151|      19|  1|15.313186646|\n",
            "|957|04APR2010|376779|      14|  1|12.782295227|\n",
            "|957|04APR2010|376779|      14|  1|5.0873527527|\n",
            "|957|04APR2010|376779|      35|  1|6.5445327759|\n",
            "|957|14AUG2011|622093|      99|  1|8.6919555664|\n",
            "|957|14AUG2011|622093|      19|  1|10.174705505|\n",
            "|957|14AUG2011|622093|       5|  1|15.236495972|\n",
            "|957|10SEP2011|639810|      99|  1|9.9497375488|\n",
            "|957|10SEP2011|639810|      35|  1|10.200271606|\n",
            "|957|10SEP2011|639810|      99|  1|6.5445327759|\n",
            "|957|10SEP2011|639810|      99|  1|2.5564575195|\n",
            "|957|10SEP2011|639810|      14|  1|6.5445327759|\n",
            "|957|10OCT2011|655931|      35|  1|10.174705505|\n",
            "|957|10OCT2011|655931|      40|  1|12.731163025|\n",
            "+---+---------+------+--------+---+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Reading orders dataframe\n",
        "orders_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../content/orders.csv\")\n",
        "\n",
        "# Show orders dataframe\n",
        "orders_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PcXHNqac5DI"
      },
      "source": [
        "### Creating a Transaction Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vy7P4jaBZC0p",
        "outputId": "7a13a185-526a-448d-92cb-79f4ffcbfece"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in the transaction dataframe: 101023\n",
            "+--------+----------+------------------+\n",
            "| cust_id|  trans_dt|         trans_amt|\n",
            "+--------+----------+------------------+\n",
            "|  100021|2009-02-25|      6.6212272644|\n",
            "|  100021|2010-01-20|     33.0805740352|\n",
            "|  100021|2010-03-06|     45.8117370608|\n",
            "|  100021|2010-04-14|        50.1065979|\n",
            "|  100021|2010-05-10|27.047328949399997|\n",
            "|  100021|2010-08-28|      50.899093628|\n",
            "|  100021|2010-09-26|      15.287620544|\n",
            "|10005188|2012-07-28|30.451723098600002|\n",
            "|10005188|2012-08-26|     45.2999877928|\n",
            "|10005188|2013-05-08|24.799987792899998|\n",
            "|10005188|2013-09-22|42.849990844000004|\n",
            "|10005188|2014-06-15|      19.799987793|\n",
            "|10005188|2014-10-04|     19.8999938964|\n",
            "|10009396|2012-08-07|     24.9506454467|\n",
            "|10009396|2013-01-05|124.44995498579999|\n",
            "|10009396|2013-01-13|      64.649982452|\n",
            "|10009396|2013-10-07|      69.900253295|\n",
            "|10009396|2014-06-01| 68.49997520360002|\n",
            "|10009396|2014-07-13| 49.69998168920001|\n",
            "|10009604|2013-05-04|     14.8999938965|\n",
            "+--------+----------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Creating a transaction dataframe\n",
        "trans_df = (\n",
        "\n",
        "    # Referencing orders dataframe\n",
        "    orders_df\n",
        "\n",
        "    # Converting the transaction date format\n",
        "    .withColumn(\"trans_dt\", f.date_format(f.to_date(f.col(\"orddate\"), \"ddMMMyyyy\"), 'yyyy-MM-dd'))\n",
        "\n",
        "    # Selecting columns\n",
        "    .select(\n",
        "        f.col(\"id\").alias(\"cust_id\"),\n",
        "        f.col(\"trans_dt\"),\n",
        "        #f.col(\"category\"),                 # Will not be used right now\n",
        "        #f.col(\"qty\").alias(\"trans_qnt\"),   # Will not be used right now\n",
        "        f.col(\"price\").alias(\"trans_amt\")\n",
        "    )\n",
        "\n",
        "    # Consider only purchase amount higher than 0. Purchases amounts <= 0 does not make sense.\n",
        "    .filter(f.col(\"trans_amt\") > 0)\n",
        "\n",
        "    # Agregating by customer id and transaction date\n",
        "    .groupBy(['cust_id', 'trans_dt'])\n",
        "    .agg(f.sum(\"trans_amt\").alias(\"trans_amt\"))\n",
        "\n",
        "    # Ordering by customer id and transaction date\n",
        "    .orderBy(f.asc(\"cust_id\"), f.asc(\"trans_dt\"))\n",
        "\n",
        "# The caching step here aims to avoid crashing the feature processing, possibly due to long DAGs.\n",
        ").cache()\n",
        "\n",
        "# Counting the number of rows and caching the dataframe\n",
        "print(f\"Number of rows in the transaction dataframe: {trans_df.count()}\")\n",
        "\n",
        "# Printing the transaction dataframe\n",
        "trans_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4uKfe14Iw1n"
      },
      "source": [
        "## Creating a Customer Lifetime value Dataframe\n",
        "\n",
        "Next we will create a customer lifetime value dataframe. This dataframe will be a representation \"snapshot\" of each customer's behavior pattern on 24 Nov 2014, which considers each customers transactions history. Historically, features that best describe customer behavior are RFML features (Recency, Frequency, Monetary and Lifetime). Therefore, we will calculate RFML features along with additional features (e.g. Rolling 6 months transaction amounts)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znKZhpcbJJjt",
        "outputId": "4eae6816-f635-4673-cd2d-96a3f8ceb4f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First transaction date across all customers: 2007-11-04\n"
          ]
        }
      ],
      "source": [
        "# Calculating the first transaction date accross all customers\n",
        "start_date = trans_df.agg({\"trans_dt\": \"min\"}).withColumnRenamed(\"min(trans_dt)\", \"date\").collect()[0].date\n",
        "\n",
        "print(f\"First transaction date across all customers: {start_date}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXhJC0f4KXVA",
        "outputId": "9d05266e-4091-4199-b9bb-a492dd2d00fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last transaction date across all customers: 2014-11-24\n"
          ]
        }
      ],
      "source": [
        "# Calculating the last transaction date accross all customers\n",
        "end_date = trans_df.agg({\"trans_dt\": \"max\"}).withColumnRenamed(\"max(trans_dt)\", \"date\").collect()[0].date\n",
        "\n",
        "print(f\"Last transaction date across all customers: {end_date}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although we have a specific task of generating predictions for purchases after 24 Nov 2014, we can actually train our model on much broader timeframe. That is, we can generate a dataframe containing information on customers behavior for each month/year available for each customer instead of using only the \"snapshot\" on 24 Nov 2014.\n",
        "\n",
        "The next feature processing step will generate such dataframe containing information on customer behavior for each valid combination of month/year and customer id."
      ],
      "metadata": {
        "id": "wt7F7KZDi9cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a PySpark calendar dataframe with daily frequency\n",
        "# Initially we will start with a daily frequency. This will be converted to a monthly frequency as features are processed in the following cells.\n",
        "calendar_df = (\n",
        "  spark.createDataFrame(pd.DataFrame({'ref_dt': pd.date_range(start=start_date, end=end_date, freq='D')}))\n",
        "  .withColumn(\"ref_dt\", f.to_date(\"ref_dt\"))\n",
        ").cache()\n",
        "\n",
        "# Counting the number of rows and caching the dataframe\n",
        "print(f\"Number of rows in the calendar dataframe: {calendar_df.count()}\")"
      ],
      "metadata": {
        "id": "rfEjViaXi1Ax",
        "outputId": "c714d7bb-4759-4220-8dc4-0cc08698fad3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in the calendar dataframe: 2578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a customer id dataframe\n",
        "# This dataframe should contain all distinct customer ids available in the transaction dataframe\n",
        "customers_id_df = (\n",
        "    trans_df\n",
        "    .select(f.col('cust_id'))\n",
        "    .distinct()\n",
        ").cache()\n",
        "\n",
        "# Counting the number of rows and caching the dataframe\n",
        "print(f\"Number of rows in the customer id dataframe: {customers_id_df.count()}\")"
      ],
      "metadata": {
        "id": "Ywcnq_loi0UN",
        "outputId": "34559d51-2501-4421-f1ad-871b80a899b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in the customer id dataframe: 16661\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VDwVHUR6XaD",
        "outputId": "c2d570b4-a19a-4836-bf2d-243d84ea389c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dataframe has: 749989 rows\n"
          ]
        }
      ],
      "source": [
        "# Creating a customer lifetime value dataframe\n",
        "cltv_df = (\n",
        "\n",
        "    # Referencing the calendar table\n",
        "    calendar_df\n",
        "\n",
        "    # Performing a crossjoin with distinct customer ids and distinct transaction dates.\n",
        "    # This should result in one record per day per customer\n",
        "    .crossJoin(customers_id_df)\n",
        "\n",
        "    # Adding aggregated transaction information about customers\n",
        "    .join(\n",
        "        trans_df\n",
        "        .withColumn(\"ref_dt\", f.col(\"trans_dt\")),\n",
        "        on=[\"cust_id\", \"ref_dt\"],\n",
        "        how=\"left\"\n",
        "    )\n",
        "\n",
        "    # Calculating the first transaction date per customer\n",
        "    .withColumn(\"first_trans_dt\", f.first(\"trans_dt\", ignorenulls=True).over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(Window.unboundedPreceding, 0)))\n",
        "\n",
        "    # Selecting only the dates after the first transaction date for each customer\n",
        "    .filter(f.col(\"ref_dt\") >= f.col(\"first_trans_dt\"))\n",
        "\n",
        "    # Calculating the last transaction date per customer\n",
        "    .withColumn(\"last_trans_dt\", f.last(\"trans_dt\", ignorenulls=True).over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(Window.unboundedPreceding, 0)))\n",
        "\n",
        "    # Calculating first transaction amount per customer\n",
        "    .withColumn(\"first_trans_amt\", f.first(\"trans_amt\", ignorenulls=True).over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(Window.unboundedPreceding, 0)))\n",
        "\n",
        "    # Calculating the lifetime of each customer: the number of days between the reference date and the first transaction date\n",
        "    .withColumn(\"lifetime\", f.datediff(f.col(\"ref_dt\"), f.col(\"first_trans_dt\")))\n",
        "\n",
        "    # Calculating the recency of each customer: the number of days between the reference date and the last transaction date\n",
        "    .withColumn(\"recency\", f.datediff(f.col(\"ref_dt\"), f.col(\"last_trans_dt\")))\n",
        "\n",
        "    # Trucating reference date to month-start (MS)\n",
        "    .withColumn(\"ref_dt\", f.trunc(f.col(\"ref_dt\"), \"month\"))\n",
        "\n",
        "    # Aggregating features\n",
        "    # Since the reference date column was truncated to its month start (MS) this aggregation will reduce signifanctly the number of records in the final dataset since we will switch from\n",
        "    # on record per day to one record per month for each customer.\n",
        "    .groupBy([\"cust_id\", \"ref_dt\", \"first_trans_amt\"])\n",
        "    .agg(\n",
        "       f.max(\"recency\").alias(\"recency\"),\n",
        "       f.sum(\"trans_amt\").alias(\"trans_amt\"),\n",
        "       f.mean(\"lifetime\").alias(\"lifetime\"),\n",
        "    )\n",
        "\n",
        "    # Converting recency from days to months\n",
        "    .withColumn(\"recency\", f.col(\"recency\")/f.lit(365/12))\n",
        "\n",
        "    # Converting lifetime from days to months\n",
        "    .withColumn(\"lifetime\", f.col(\"lifetime\")/f.lit(365/12))\n",
        "\n",
        "    # Adjusting the transaction amount column. This step adds the value zero to the transaction amount column for each month/customer without transaction\n",
        "    .withColumn(\"trans_amt\", f.when(f.col(\"trans_amt\") > 0, f.col(\"trans_amt\")).otherwise(f.lit(0)))\n",
        "\n",
        "    # Creating a transaction indicator\n",
        "    .withColumn(\"trans_ind\", f.when(f.col(\"trans_amt\") > 0, f.lit(1)).otherwise(f.lit(0)))\n",
        "\n",
        "    # Calculating Frequency: The number of months with transactions per customer between the firt transaction date and the reference date\n",
        "    .withColumn(\"frequency\", f.sum(\"trans_ind\").over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(Window.unboundedPreceding, 0)))\n",
        "\n",
        "    # Calculating Monetary: The total amount spent per customer between the firt transaction date and the reference date\n",
        "    .withColumn(\"monetary\", f.sum(\"trans_amt\").over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(Window.unboundedPreceding, 0)))\n",
        "\n",
        "    # Calculating transaction rate: the average number of transactions per month\n",
        "    .withColumn(\"trans_rate\", f.when(f.col(\"lifetime\") == 0, f.lit(1)).otherwise(f.col(\"frequency\") / f.col(\"lifetime\")))\n",
        "\n",
        "    # Calculating rolling 6 months transaction quantity\n",
        "    .withColumn(\"trans_qnt_R06m\",        f.sum(\"trans_ind\").over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(-5, 0)))\n",
        "    .withColumn(\"trans_qnt_R06m_lag1\",   f.sum(\"trans_ind\").over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(-11, -6)))\n",
        "    .withColumn(\"trans_qnt_R06m_lag2\",   f.sum(\"trans_ind\").over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(-17, -12)))\n",
        "\n",
        "    # Calculating rolling 6 months transaction amount\n",
        "    .withColumn(\"trans_amt_R06m\",       f.sum(\"trans_amt\").over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(-5, 0)))\n",
        "    .withColumn(\"trans_amt_R06m_lag1\",  f.sum(\"trans_amt\").over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(-11, -6)))\n",
        "    .withColumn(\"trans_amt_R06m_lag2\",  f.sum(\"trans_amt\").over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(-17, -12)))\n",
        "\n",
        "    # Calculating the transaction amount for next 6 months\n",
        "    # This column can be used as a target column (as an alternative for logtarg) for model training\n",
        "    .withColumn(\"trans_amt_R06m_lag2\",  f.sum(\"trans_amt\").over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(-17, -12)))\n",
        "\n",
        "    # Calculating rolling 6 months moving average of the transaction quantity\n",
        "    .withColumn(\"trans_qnt_R06m_mov_avg\", f.mean(\"trans_qnt_R06m\").over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(Window.unboundedPreceding, 0)))\n",
        "\n",
        "    # Calculating rolling 6 months moving average of the transaction amount\n",
        "    .withColumn(\"trans_amt_R06m_mov_avg\", f.mean(\"trans_amt_R06m\").over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(Window.unboundedPreceding, 0)))\n",
        "\n",
        "    # Fill missing values with -1\n",
        "    # This is step will add -1 to the rolling 6 months calculations in cases where there is not enough transaction history to calculate the corresponding feature\n",
        "    .fillna(-1)\n",
        "\n",
        "    # Ordering and selecting columns\n",
        "    .select(\n",
        "      \"cust_id\",\n",
        "      \"ref_dt\",\n",
        "      \"first_trans_amt\",\n",
        "      \"trans_rate\",\n",
        "      \"recency\",\n",
        "      \"frequency\",\n",
        "      \"monetary\",\n",
        "      \"lifetime\",\n",
        "      \"trans_qnt_R06m\",\n",
        "      \"trans_qnt_R06m_lag1\",\n",
        "      \"trans_qnt_R06m_lag2\",\n",
        "      \"trans_qnt_R06m_mov_avg\",\n",
        "      \"trans_amt_R06m\",\n",
        "      \"trans_amt_R06m_lag1\",\n",
        "      \"trans_amt_R06m_lag2\",\n",
        "      \"trans_amt_R06m_mov_avg\"\n",
        "    )\n",
        "\n",
        "    # Ordering by customer id and transaction date\n",
        "    .orderBy(f.asc(\"cust_id\"), f.asc(\"ref_dt\"))\n",
        "\n",
        ").cache()\n",
        "\n",
        "print(f\"The dataframe has: {cltv_df.count()} rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfpkJeHqBBH_",
        "outputId": "3122d98f-71b8-41d4-fcfa-0014290a504c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+---------------+-------------------+-------------------+---------+------------------+--------------------+--------------+-------------------+-------------------+----------------------+------------------+-------------------+-------------------+----------------------+\n",
            "|cust_id|    ref_dt|first_trans_amt|         trans_rate|            recency|frequency|          monetary|            lifetime|trans_qnt_R06m|trans_qnt_R06m_lag1|trans_qnt_R06m_lag2|trans_qnt_R06m_mov_avg|    trans_amt_R06m|trans_amt_R06m_lag1|trans_amt_R06m_lag2|trans_amt_R06m_mov_avg|\n",
            "+-------+----------+---------------+-------------------+-------------------+---------+------------------+--------------------+--------------+-------------------+-------------------+----------------------+------------------+-------------------+-------------------+----------------------+\n",
            "| 100021|2009-02-01|   6.6212272644|  20.27777777777778|0.09863013698630137|        1|      6.6212272644|0.049315068493150684|             1|                 -1|                 -1|                   1.0|      6.6212272644|               -1.0|               -1.0|          6.6212272644|\n",
            "| 100021|2009-03-01|   6.6212272644| 1.6008771929824561|  1.117808219178082|        1|      6.6212272644|  0.6246575342465753|             1|                 -1|                 -1|                   1.0|      6.6212272644|               -1.0|               -1.0|          6.6212272644|\n",
            "| 100021|2009-04-01|   6.6212272644| 0.6144781144781145| 2.1041095890410957|        1|      6.6212272644|  1.6273972602739726|             1|                 -1|                 -1|                   1.0|      6.6212272644|               -1.0|               -1.0|     6.621227264400001|\n",
            "| 100021|2009-05-01|   6.6212272644|0.38020833333333337| 3.1232876712328768|        1|      6.6212272644|  2.6301369863013697|             1|                 -1|                 -1|                   1.0|      6.6212272644|               -1.0|               -1.0|          6.6212272644|\n",
            "| 100021|2009-06-01|   6.6212272644|   0.27526395173454|   4.10958904109589|        1|      6.6212272644|   3.632876712328767|             1|                 -1|                 -1|                   1.0|      6.6212272644|               -1.0|               -1.0|          6.6212272644|\n",
            "| 100021|2009-07-01|   6.6212272644| 0.2157210401891253|  5.128767123287671|        1|      6.6212272644|   4.635616438356164|             1|                 -1|                 -1|                   1.0|      6.6212272644|               -1.0|               -1.0|     6.621227264399999|\n",
            "| 100021|2009-08-01|   6.6212272644|0.17684108527131784|  6.147945205479452|        1|      6.6212272644|   5.654794520547945|             0|                  1|                 -1|    0.8571428571428571|               0.0|       6.6212272644|               -1.0|     5.675337655199999|\n",
            "| 100021|2009-09-01|   6.6212272644|0.15020576131687244|  7.134246575342465|        1|      6.6212272644|   6.657534246575342|             0|                  1|                 -1|                  0.75|               0.0|       6.6212272644|               -1.0|    4.9659204482999995|\n",
            "| 100021|2009-10-01|   6.6212272644|0.13054363376251787|  8.153424657534247|        1|      6.6212272644|    7.66027397260274|             0|                  1|                 -1|    0.6666666666666666|               0.0|       6.6212272644|               -1.0|          4.4141515096|\n",
            "| 100021|2009-11-01|   6.6212272644|0.11543327008222645|   9.13972602739726|        1|      6.6212272644|   8.663013698630136|             0|                  1|                 -1|                   0.6|               0.0|       6.6212272644|               -1.0|    3.9727363586399997|\n",
            "| 100021|2009-12-01|   6.6212272644|0.10345804988662131| 10.158904109589042|        1|      6.6212272644|   9.665753424657535|             0|                  1|                 -1|    0.5454545454545454|               0.0|       6.6212272644|               -1.0|    3.6115785078545453|\n",
            "| 100021|2010-01-01|   6.6212272644|0.18717948717948718| 10.783561643835617|        2|     39.7018012996|  10.684931506849315|             1|                  1|                 -1|    0.5833333333333334|     33.0805740352|       6.6212272644|               -1.0|     6.067328135133333|\n",
            "| 100021|2010-02-01|   6.6212272644|0.17160319699106724| 1.2821917808219176|        2|     39.7018012996|  11.654794520547945|             1|                  0|                  1|    0.6153846153846154|     33.0805740352|                0.0|       6.6212272644|     8.145270127446153|\n",
            "| 100021|2010-03-01|   6.6212272644|0.23763020833333334| 1.4465753424657535|        3| 85.51353836039999|  12.624657534246575|             2|                  0|                  1|    0.7142857142857143|      78.892311096|                0.0|       6.6212272644|     13.19863019662857|\n",
            "| 100021|2010-04-01|   6.6212272644| 0.2935263369521512| 1.2493150684931507|        4|    135.6201362604|  13.627397260273971|             3|                  0|                  1|    0.8666666666666667|     128.998908996|                0.0|       6.6212272644|    20.918648783253335|\n",
            "| 100021|2010-05-01|   6.6212272644|0.34176029962546817|  0.821917808219178|        5|162.66746520979999|   14.63013698630137|             4|                  0|                  1|                1.0625|    156.0462379454|                0.0|       6.6212272644|      29.3641231058875|\n",
            "| 100021|2010-06-01|   6.6212272644|  0.319838766211006| 1.6767123287671233|        5|162.66746520979999|  15.632876712328766|             4|                  0|                  1|    1.2352941176470589|    156.0462379454|                0.0|       6.6212272644|    36.816012214094116|\n",
            "| 100021|2010-07-01|   6.6212272644|0.30055994729907776|  2.695890410958904|        5|162.66746520979999|  16.635616438356163|             3|                  1|                  1|    1.3333333333333333|122.96566391019999|      33.0805740352|       6.6212272644|    41.602103974988886|\n",
            "| 100021|2010-08-01|   6.6212272644| 0.3398510242085661| 3.5835616438356164|        6|    213.5665588378|  17.654794520547945|             4|                  1|                  0|    1.4736842105263157|    173.8647575382|      33.0805740352|                0.0|    48.563296267789475|\n",
            "| 100021|2010-09-01|   6.6212272644|0.37518355359765054| 0.9205479452054794|        7|228.85417938179998|   18.65753424657534|             4|                  2|                  0|                   1.6|    143.3406410214|       78.892311096|                0.0|    53.302163505470006|\n",
            "+-------+----------+---------------+-------------------+-------------------+---------+------------------+--------------------+--------------+-------------------+-------------------+----------------------+------------------+-------------------+-------------------+----------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Printing a sample of the dataset\n",
        "cltv_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5p6HDaA0YZJ"
      },
      "source": [
        "## Storing the Processed Dataset as Parquet Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Nwa59wh9yHj-"
      },
      "outputs": [],
      "source": [
        "# Defining dataframe path\n",
        "file_path = \"../content/drive/MyDrive/Colab/Sandbox/customer_spend_model\"\n",
        "\n",
        "# Storing Customer Lifetime Value Data\n",
        "cltv_df.write.format(\"parquet\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(file_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}