{"cells":[{"cell_type":"markdown","metadata":{"id":"qsOcHa0Slj9J"},"source":["## Installing Kaggle API"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":7935,"status":"ok","timestamp":1722044008279,"user":{"displayName":"Felipe Tufaile","userId":"10526460027904460804"},"user_tz":180},"id":"YAM8BySSljtB","outputId":"4ea93b1e-7442-4cf5-8efb-61be4026248f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.12)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n","Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.2)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n"]}],"source":["# Installing Kaggle API\n","!pip install kaggle"]},{"cell_type":"markdown","metadata":{"id":"Yy4uRUwRlwYw"},"source":["## Configuring Kaggle Credentials"]},{"cell_type":"markdown","metadata":{"id":"wnlj-vyAlU9t"},"source":["Setup kaggle API correctly following https://www.kaggle.com/docs/api\n","```\n","%%shell\n","mkdir ~/.kaggle\n","echo \\{\\\"username\\\":\\\"{your kaggle username}\\\",\\\"key\\\":\\\"{your kaggle api key}\\\"\\} > ~/.kaggle/kaggle.json\n","pip install kaggle\n","```"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":5887,"status":"ok","timestamp":1722044014152,"user":{"displayName":"Felipe Tufaile","userId":"10526460027904460804"},"user_tz":180},"id":"A6whITUBl1DL","outputId":"1b1f7bb3-c64b-4482-e8e1-d160b0448dbf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.12)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n","Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.2)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n"]},{"data":{"text/plain":[]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["%%shell\n","mkdir ~/.kaggle\n","echo \\{\\\"username\\\":\\\"{your kaggle username}\\\",\\\"key\\\":\\\"{your kaggle api key}\\\"\\} > ~/.kaggle/kaggle.json\n","pip install kaggle"]},{"cell_type":"markdown","metadata":{"id":"cBsgg36gp_F8"},"source":["## Installing PySpark\n","\n","Since part of the feature engineering process will involve dealing with large amounts of data, we will be using PySpark.\n","To facilitate future work, the final processed table will be stored as a parquet file in Google Drive."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":799},"collapsed":true,"executionInfo":{"elapsed":73041,"status":"ok","timestamp":1722044091381,"user":{"displayName":"Felipe Tufaile","userId":"10526460027904460804"},"user_tz":180},"id":"TJYSevZzp8Yu","outputId":"016f6c37-e3f7-40d8-9953-954346512f9f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n","Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n","Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [859 kB]\n","Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n","Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [55.6 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n","Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n","Get:9 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,104 kB]\n","Get:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n","Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n","Get:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [51.8 kB]\n","Get:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [24.6 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,420 kB]\n","Get:16 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [27.8 kB]\n","Get:17 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,771 kB]\n","Get:18 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [48.1 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,378 kB]\n","Get:20 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.7 kB]\n","Get:21 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,129 kB]\n","Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,858 kB]\n","Get:23 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [33.7 kB]\n","Get:24 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [81.0 kB]\n","Fetched 14.3 MB in 3s (4,306 kB/s)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","114 packages can be upgraded. Run 'apt list --upgradable' to see them.\n","tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n","tar: Error is not recoverable: exiting now\n","Collecting pyspark\n","  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=9b570114767071187434dd75420fc72da27621d21ae2c146816c22b96e660658\n","  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.5.1\n","Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/usr/local/lib/python3.10/dist-packages/pyspark'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["!sudo apt update\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n","!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n","!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n","!pip install -q findspark\n","!pip install pyspark\n","!pip install py4j\n","\n","import os\n","import sys\n","# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n","\n","\n","import findspark\n","findspark.init()\n","findspark.find()"]},{"cell_type":"markdown","metadata":{"id":"iG2a5JhMRwpd"},"source":["## Loading Libraries"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"executionInfo":{"elapsed":8491,"status":"ok","timestamp":1722044099860,"user":{"displayName":"Felipe Tufaile","userId":"10526460027904460804"},"user_tz":180},"id":"sJGlwXgQRznh","outputId":"f441e8a9-cb85-4e1a-8b1b-7b7297db8975"},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://dae85f19e98b:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.5.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>DataProcessingPySpark</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x786ca6899360>"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Importing Numpy library\n","import numpy as np\n","\n","# Importing Pandas library\n","import pandas as pd\n","\n","# Importing datetime library\n","from datetime import datetime, timedelta\n","\n","# importing the zipfile module\n","from zipfile import ZipFile\n","\n","# Importing gzip library\n","import gzip\n","\n","# Importing plotying libraries\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Importing userdata from Google Colab Library\n","from google.colab import userdata\n","\n","# Importing pyspark libraries\n","from pyspark.sql import SparkSession\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType, TimestampType\n","import pyspark.sql.functions as f\n","from pyspark.sql.window import Window\n","\n","# Creating a spark session\n","spark = SparkSession.builder.appName(\"DataProcessingPySpark\").getOrCreate()\n","\n","spark"]},{"cell_type":"markdown","metadata":{"id":"Jnd96QbXl8X4"},"source":["## Downloading Customer Spend Model Data\n","\n","This notebook use the customer spend model data available in the Kaggle. The dataset can be found in the following link: https://www.kaggle.com/competitions/customer-spend-model/data.\n","The task requested by the author of the competition is to **predict customer purchases from German book company**. In order to do so, we are provided with the transaction history for each customer up to Nov 24, 2014.\n","The output that we are requested to provide is the log of the estimated amount spent by each customer after Nov 24, 2014. In order to avoid log(0), we are requested to adjust the log calculation to log(x+1), where x is the estimate of the future amount spent.\n","For more details about the task description you can access the following link: https://www.kaggle.com/competitions/customer-spend-model/overview."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3078,"status":"ok","timestamp":1722044102871,"user":{"displayName":"Felipe Tufaile","userId":"10526460027904460804"},"user_tz":180},"id":"ykfl_kK2h5I-","outputId":"08d5ce93-3c3a-4573-ea2f-b2e66b84bc73"},"outputs":[{"name":"stdout","output_type":"stream","text":["Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n","Downloading customer-spend-model.zip to /content\n"," 89% 2.00M/2.24M [00:00<00:00, 2.89MB/s]\n","100% 2.24M/2.24M [00:00<00:00, 2.78MB/s]\n"]}],"source":["# Downloading the Customer Spend Model Data from Kaggle\n","!kaggle competitions download -c customer-spend-model"]},{"cell_type":"markdown","metadata":{"id":"kB-ZPtPporjr"},"source":["## Unzipping the Transactions Dataset\n","\n","This is step is necessary to unzip the csv file download in the previous cell."]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1722044102872,"user":{"displayName":"Felipe Tufaile","userId":"10526460027904460804"},"user_tz":180},"id":"jhIt3xqLmnFG"},"outputs":[],"source":["# loading the customer-spend-model.zip and creating a zip object\n","with ZipFile(\"../content/customer-spend-model.zip\", 'r') as zip_object:\n","\n","\t# Extracting specific file in the zip into a specific location.\n","\tzip_object.extract(\"customer.csv\", path=\"../content\")\n","\n"," \t# Extracting specific file in the zip into a specific location.\n","\tzip_object.extract(\"orders.csv\", path=\"../content\")\n","\n","# closing object\n","zip_object.close()"]},{"cell_type":"markdown","metadata":{"id":"3qecZproxKwV"},"source":["## Processing Data Using PySpark"]},{"cell_type":"markdown","metadata":{},"source":["### Loading Orders Dataframe\n","\n","**orders.csv**: all orders prior to 11/25/2014 for training (n=5,551) and test (n=11,230) sets. You should find 353,687 records plus a header.\n","\n","- **id**: unique customer identifier;\n","- **orddate**: order date;\n","- **ordnum**: order number;\n","- **category**: \n","    - category identifier:, \n","        - 1 = fiction; \n","        - 3 = classics; \n","        - 5 = cartoons; \n","        - 6 = legends; \n","        - 7 = philosophy; \n","        - 8 = religion; \n","        - 9 = psychology; \n","        - 10 = linguistics; \n","        - 12 = art; \n","        - 14 = music; \n","        - 17 = art reprints; \n","        - 19 = history; \n","        - 20 = contemporary history; \n","        - 21 = economy; \n","        - 22 = politics; \n","        - 23 = science; \n","        - 26 = computer science; \n","        - 27 = traffic, railroads; \n","        - 30 = maps; \n","        - 31 = travel guides; \n","        - 35 = health; \n","        - 36 = cooking; \n","        - 37 = learning; \n","        - 38 = games and riddles; \n","        - 39 = sports; \n","        - 40 = hobby; \n","        - 41 = nature/animals/plants; \n","        - 44 = encyclopedia; \n","        - 50 = videos, DVDs; \n","        - 99 = non books\n","- **qty**: quantity;\n","- **price**: price paid;"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6506,"status":"ok","timestamp":1722044134964,"user":{"displayName":"Felipe Tufaile","userId":"10526460027904460804"},"user_tz":180},"id":"cpa6qQkDXeAm","outputId":"dc28927b-5b54-4883-9ee4-9ffb6d9fa893"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+---------+------+--------+---+------------+\n","| id|  orddate|ordnum|category|qty|       price|\n","+---+---------+------+--------+---+------------+\n","|957|10FEB2008| 38650|      35|  1|5.0106582642|\n","|957|10FEB2008| 38650|      35|  1|20.426101685|\n","|957|10FEB2008| 38650|      19|  1|20.400543213|\n","|957|15MAR2008| 48972|      40|  1|25.539016724|\n","|957|22NOV2008|150011|      40|  1|14.316169739|\n","|957|22NOV2008|150011|      40|  1|8.5896987915|\n","|957|03OCT2009|286151|      19|  1|15.313186646|\n","|957|04APR2010|376779|      14|  1|12.782295227|\n","|957|04APR2010|376779|      14|  1|5.0873527527|\n","|957|04APR2010|376779|      35|  1|6.5445327759|\n","|957|14AUG2011|622093|      99|  1|8.6919555664|\n","|957|14AUG2011|622093|      19|  1|10.174705505|\n","|957|14AUG2011|622093|       5|  1|15.236495972|\n","|957|10SEP2011|639810|      99|  1|9.9497375488|\n","|957|10SEP2011|639810|      35|  1|10.200271606|\n","|957|10SEP2011|639810|      99|  1|6.5445327759|\n","|957|10SEP2011|639810|      99|  1|2.5564575195|\n","|957|10SEP2011|639810|      14|  1|6.5445327759|\n","|957|10OCT2011|655931|      35|  1|10.174705505|\n","|957|10OCT2011|655931|      40|  1|12.731163025|\n","+---+---------+------+--------+---+------------+\n","only showing top 20 rows\n","\n"]}],"source":["# Reading orders dataframe\n","orders_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../content/orders.csv\")\n","\n","# Show orders dataframe\n","orders_df.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Creating a Transaction Dataframe"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3459,"status":"ok","timestamp":1722044993576,"user":{"displayName":"Felipe Tufaile","userId":"10526460027904460804"},"user_tz":180},"id":"Vy7P4jaBZC0p","outputId":"d4027b67-5a4b-41fa-cc5e-ab70224431e7"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+----------+------------------+\n","| cust_id|  trans_dt|         trans_amt|\n","+--------+----------+------------------+\n","|  100021|2009-02-25|      6.6212272644|\n","|  100021|2010-01-20|     33.0805740352|\n","|  100021|2010-03-06|     45.8117370608|\n","|  100021|2010-04-14|        50.1065979|\n","|  100021|2010-05-10|27.047328949399997|\n","|  100021|2010-08-28|      50.899093628|\n","|  100021|2010-09-26|      15.287620544|\n","|10005188|2012-07-28|30.451723098600002|\n","|10005188|2012-08-26|     45.2999877928|\n","|10005188|2013-05-08|24.799987792899998|\n","|10005188|2013-09-22|42.849990844000004|\n","|10005188|2014-06-15|      19.799987793|\n","|10005188|2014-10-04|     19.8999938964|\n","|10009396|2012-08-07|     24.9506454467|\n","|10009396|2013-01-05|124.44995498579999|\n","|10009396|2013-01-13|      64.649982452|\n","|10009396|2013-10-07|      69.900253295|\n","|10009396|2014-06-01| 68.49997520360002|\n","|10009396|2014-07-13| 49.69998168920001|\n","|10009604|2013-05-04|     14.8999938965|\n","+--------+----------+------------------+\n","only showing top 20 rows\n","\n"]}],"source":["# Creating a transaction dataframe\n","trans_df = (\n","\n","    # Referencing orders dataframe\n","    orders_df\n","\n","    # Converting the transaction date format\n","    .withColumn(\"trans_dt\", f.date_format(f.to_date(f.col(\"orddate\"), \"ddMMMyyyy\"), 'yyyy-MM-dd'))\n","\n","    # Selecting columns\n","    .select(\n","        f.col(\"id\").alias(\"cust_id\"),\n","        f.col(\"trans_dt\"),\n","        #f.col(\"category\"),                 # Will not be used right now\n","        #f.col(\"qty\").alias(\"trans_qnt\"),   # Will not be used right now\n","        f.col(\"price\").alias(\"trans_amt\")\n","    )\n","\n","    # Consider only purchase amount higher than 0. Purchases amounts <= 0 does not make sense.\n","    .filter(f.col(\"trans_amt\") > 0)\n","\n","    # Agregating by customer id and transaction date\n","    .groupBy(['cust_id', 'trans_dt'])\n","    .agg(f.sum(\"trans_amt\").alias(\"trans_amt\"))\n","\n","    # Ordering by customer id and transaction date\n","    .orderBy(f.asc(\"cust_id\"), f.asc(\"trans_dt\"))\n","\n","# The caching step here aims to avoid crashing the feature processing, possibly due to long DAGs.\n",").cache()\n","\n","# Printing the transaction dataframe\n","trans_df.show()"]},{"cell_type":"markdown","metadata":{"id":"T4uKfe14Iw1n"},"source":["## Creating a Cross-Join Dataframe\n","\n","Next we will create a cross-join dataframe between transaction date and customer id. The result of this process will be a dataframe with one record per day per customer id.\n","This dataset format helps generating more \"snapshots\"  "]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1467,"status":"ok","timestamp":1722044998702,"user":{"displayName":"Felipe Tufaile","userId":"10526460027904460804"},"user_tz":180},"id":"znKZhpcbJJjt","outputId":"6bf8243c-ecb4-4d5f-d879-ee83cfe921fa"},"outputs":[{"name":"stdout","output_type":"stream","text":["First transaction date across all customers: 2007-11-04\n"]}],"source":["# Calculating the first transaction date accross all customers\n","start_date = trans_df.agg({\"trans_dt\": \"min\"}).withColumnRenamed(\"min(trans_dt)\", \"date\").collect()[0].date\n","\n","print(f\"First transaction date across all customers: {start_date}\")"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":526,"status":"ok","timestamp":1722044999695,"user":{"displayName":"Felipe Tufaile","userId":"10526460027904460804"},"user_tz":180},"id":"FXhJC0f4KXVA","outputId":"8daebcd1-7872-4766-925a-e94e53d7419c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Last transaction date across all customers: 2014-11-24\n"]}],"source":["# Calculating the last transaction date accross all customers\n","end_date = trans_df.agg({\"trans_dt\": \"max\"}).withColumnRenamed(\"max(trans_dt)\", \"date\").collect()[0].date\n","\n","print(f\"Last transaction date across all customers: {end_date}\")"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":1159,"status":"ok","timestamp":1722045001648,"user":{"displayName":"Felipe Tufaile","userId":"10526460027904460804"},"user_tz":180},"id":"f7gKfEJMPHjf"},"outputs":[],"source":["# Create a PySpark calendar dataframe with month-start frequency\n","calendar_df = (\n","  spark.createDataFrame(pd.DataFrame({'ref_dt': pd.date_range(start=start_date, end=end_date, freq='D')}))\n","  .withColumn(\"ref_dt\", f.to_date(\"ref_dt\"))\n",").cache()"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1722045002382,"user":{"displayName":"Felipe Tufaile","userId":"10526460027904460804"},"user_tz":180},"id":"J5vxcF6k3DQw"},"outputs":[],"source":["# Creating a customer id dataframe\n","customers_id_df = (\n","    trans_df\n","    .select(f.col('cust_id'))\n","    .distinct()\n",").cache()"]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1882,"status":"ok","timestamp":1722047334394,"user":{"displayName":"Felipe Tufaile","userId":"10526460027904460804"},"user_tz":180},"id":"7VDwVHUR6XaD","outputId":"98ee0574-cffe-4a4e-a55c-2a7fe35afab3"},"outputs":[{"name":"stdout","output_type":"stream","text":["The dataframe has: 749989 rows\n"]}],"source":["# Creating a customer lifetime value dataframe\n","cltv_df = (\n","\n","    # Referencing the calendar table\n","    calendar_df\n","\n","    # Performing a crossjoin with distinct customer ids and distinct transaction dates.\n","    # This should result in one record per month/year per customer\n","    .crossJoin(customers_id_df)\n","\n","    # Adding aggregated transaction information about customers\n","    .join(\n","        trans_df\n","        .withColumn(\"ref_dt\", f.col(\"trans_dt\")),\n","        on=[\"cust_id\", \"ref_dt\"],\n","        how=\"left\"\n","    )\n","\n","    # Calculating the first transaction date per customer\n","    .withColumn(\"first_trans_dt\", f.first(\"trans_dt\", ignorenulls=True).over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(Window.unboundedPreceding, 0)))\n","\n","    # Selecting only the dates after the first transaction date for each customer\n","    .filter(f.col(\"ref_dt\") >= f.col(\"first_trans_dt\"))\n","\n","    # Calculating last transaction date columns: overwriting null values\n","    .withColumn(\"last_trans_dt\", f.last(\"trans_dt\", ignorenulls=True).over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(Window.unboundedPreceding, 0)))\n","\n","    # Calculating first purchase amount\n","    .withColumn(\"first_trans_amt\", f.first(\"trans_amt\", ignorenulls=True).over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(Window.unboundedPreceding, 0)))\n","\n","    # Calculating account age: the number of days since the first purchase\n","    .withColumn(\"lifetime\", f.datediff(f.col(\"ref_dt\"), f.col(\"first_trans_dt\")))\n","\n","    # Calculating recency: the number of days since the last transaction date\n","    .withColumn(\"recency\", f.datediff(f.col(\"ref_dt\"), f.col(\"last_trans_dt\")))\n","\n","    # Trucating reference date to month-start\n","    .withColumn(\"ref_dt\", f.trunc(f.col(\"ref_dt\"), \"month\"))\n","\n","    # Aggregating RFML data\n","    .groupBy([\"cust_id\", \"ref_dt\", \"first_trans_amt\"])\n","    .agg(\n","       f.max(\"recency\").alias(\"recency\"),\n","       f.sum(\"trans_amt\").alias(\"trans_amt\"),\n","       f.mean(\"lifetime\").alias(\"lifetime\"),\n","    )\n","\n","    # Adjusting the transaction amount column\n","    .withColumn(\"trans_amt\", f.when(f.col(\"trans_amt\") > 0, f.col(\"trans_amt\")).otherwise(f.lit(0)))\n","\n","    # Creating a transaction indicator\n","    .withColumn(\"trans_ind\", f.when(f.col(\"trans_amt\") > 0, f.lit(1)).otherwise(f.lit(0)))\n","\n","    # Calculating Frequency: total number of transactions until the current date\n","    .withColumn(\"frequency\", f.sum(\"trans_ind\").over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(Window.unboundedPreceding, 0)))\n","\n","    # Calculating Monetary: total purchase amount until the current date\n","    .withColumn(\"monetary\", f.sum(\"trans_amt\").over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(Window.unboundedPreceding, 0)))\n","\n","    # Calculating transaction rate: the average number of transactions per day\n","    .withColumn(\"trans_rate\", f.when(f.col(\"lifetime\") == 0, f.lit(1)).otherwise(f.col(\"frequency\") / f.col(\"lifetime\")))\n","\n","    # Calculating rolling purchase quantity\n","    .withColumn(\"trans_qnt_R06m\",        f.sum(\"trans_ind\").over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(-5, 0)))\n","    .withColumn(\"trans_qnt_R06m_lag1\",  f.sum(\"trans_ind\").over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(-11, -5)))\n","    .withColumn(\"trans_qnt_R06m_lag2\",  f.sum(\"trans_ind\").over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(-17, -11)))\n","\n","    # Calculating rolling purchase amount\n","    .withColumn(\"trans_amt_R06m\",       f.sum(\"trans_amt\").over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(-5, 0)))\n","    .withColumn(\"trans_amt_R06m_lag1\",  f.sum(\"trans_amt\").over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(-11, -5)))\n","    .withColumn(\"trans_amt_R06m_lag2\",  f.sum(\"trans_amt\").over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(-17, -11)))\n","\n","    # Calculating rolling purchase quantity value 30 days moving average\n","    .withColumn(\"trans_qnt_R06m_mov_avg\", f.mean(\"trans_qnt_R06m\").over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(Window.unboundedPreceding, 0)))\n","\n","    # Calculating rolling purchase amount value 30 days moving average\n","    .withColumn(\"trans_amt_R06m_mov_avg\", f.mean(\"trans_amt_R06m\").over(Window.partitionBy([\"cust_id\"]).orderBy(f.asc(\"ref_dt\")).rowsBetween(Window.unboundedPreceding, 0)))\n","\n","    # Fill missing values with -1\n","    .fillna(-1)\n","\n","    # Ordering and selecting columns\n","    .select(\n","      \"cust_id\",\n","      \"ref_dt\",\n","      \"first_trans_amt\",\n","      \"trans_rate\",\n","      \"recency\",\n","      \"frequency\",\n","      \"monetary\",\n","      \"lifetime\",\n","      \"trans_qnt_R06m\",\n","      \"trans_qnt_R06m_lag1\",\n","      \"trans_qnt_R06m_lag2\",\n","      \"trans_qnt_R06m_mov_avg\",\n","      \"trans_amt_R06m\",\n","      \"trans_amt_R06m_lag1\",\n","      \"trans_amt_R06m_lag2\",\n","      \"trans_amt_R06m_mov_avg\"\n","    )\n","\n","    # Adding train \\ test information\n","    .join(spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../content/customer.csv\").withColumnRenamed(\"id\", \"cust_id\"), how=\"left\", on=[\"cust_id\"])\n","\n","    # Ordering by customer id and transaction date\n","    .orderBy(f.asc(\"cust_id\"), f.asc(\"ref_dt\"))\n","\n",").cache()\n","\n","print(f\"The dataframe has: {trans_cross_df.count()} rows\")"]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7587,"status":"ok","timestamp":1722047362831,"user":{"displayName":"Felipe Tufaile","userId":"10526460027904460804"},"user_tz":180},"id":"AfpkJeHqBBH_","outputId":"5ae48c8c-9374-4b11-84d3-d7d1daa80f44"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+----------+---------------+--------------------+-------+---------+------------------+--------+--------------+-------------------+-------------------+----------------------+------------------+-------------------+-------------------+----------------------+-----+-------+\n","|cust_id|    ref_dt|first_trans_amt|          trans_rate|recency|frequency|          monetary|lifetime|trans_qnt_R06m|trans_qnt_R06m_lag1|trans_qnt_R06m_lag2|trans_qnt_R06m_mov_avg|    trans_amt_R06m|trans_amt_R06m_lag1|trans_amt_R06m_lag2|trans_amt_R06m_mov_avg|train|logtarg|\n","+-------+----------+---------------+--------------------+-------+---------+------------------+--------+--------------+-------------------+-------------------+----------------------+------------------+-------------------+-------------------+----------------------+-----+-------+\n","| 100021|2009-02-01|   6.6212272644|  0.6666666666666666|      3|        1|      6.6212272644|     1.5|             1|                 -1|                 -1|                   1.0|      6.6212272644|               -1.0|               -1.0|          6.6212272644|    1|      0|\n","| 100021|2009-03-01|   6.6212272644| 0.05263157894736842|     34|        1|      6.6212272644|    19.0|             1|                 -1|                 -1|                   1.0|      6.6212272644|               -1.0|               -1.0|          6.6212272644|    1|      0|\n","| 100021|2009-04-01|   6.6212272644|0.020202020202020204|     64|        1|      6.6212272644|    49.5|             1|                 -1|                 -1|                   1.0|      6.6212272644|               -1.0|               -1.0|     6.621227264400001|    1|      0|\n","| 100021|2009-05-01|   6.6212272644|              0.0125|     95|        1|      6.6212272644|    80.0|             1|                 -1|                 -1|                   1.0|      6.6212272644|               -1.0|               -1.0|          6.6212272644|    1|      0|\n","| 100021|2009-06-01|   6.6212272644| 0.00904977375565611|    125|        1|      6.6212272644|   110.5|             1|                 -1|                 -1|                   1.0|      6.6212272644|               -1.0|               -1.0|          6.6212272644|    1|      0|\n","| 100021|2009-07-01|   6.6212272644|0.007092198581560...|    156|        1|      6.6212272644|   141.0|             1|                  1|                 -1|                   1.0|      6.6212272644|       6.6212272644|               -1.0|     6.621227264399999|    1|      0|\n","| 100021|2009-08-01|   6.6212272644|0.005813953488372093|    187|        1|      6.6212272644|   172.0|             0|                  1|                 -1|    0.8571428571428571|               0.0|       6.6212272644|               -1.0|     5.675337655199999|    1|      0|\n","| 100021|2009-09-01|   6.6212272644|0.004938271604938...|    217|        1|      6.6212272644|   202.5|             0|                  1|                 -1|                  0.75|               0.0|       6.6212272644|               -1.0|    4.9659204482999995|    1|      0|\n","| 100021|2009-10-01|   6.6212272644|0.004291845493562232|    248|        1|      6.6212272644|   233.0|             0|                  1|                 -1|    0.6666666666666666|               0.0|       6.6212272644|               -1.0|          4.4141515096|    1|      0|\n","| 100021|2009-11-01|   6.6212272644|0.003795066413662239|    278|        1|      6.6212272644|   263.5|             0|                  1|                 -1|                   0.6|               0.0|       6.6212272644|               -1.0|    3.9727363586399997|    1|      0|\n","| 100021|2009-12-01|   6.6212272644|0.003401360544217687|    309|        1|      6.6212272644|   294.0|             0|                  1|                 -1|    0.5454545454545454|               0.0|       6.6212272644|               -1.0|    3.6115785078545453|    1|      0|\n","| 100021|2010-01-01|   6.6212272644|0.006153846153846154|    328|        2|     39.7018012996|   325.0|             1|                  1|                  1|    0.5833333333333334|     33.0805740352|       6.6212272644|       6.6212272644|     6.067328135133333|    1|      0|\n","| 100021|2010-02-01|   6.6212272644|0.005641748942172073|     39|        2|     39.7018012996|   354.5|             1|                  0|                  1|    0.6153846153846154|     33.0805740352|                0.0|       6.6212272644|     8.145270127446153|    1|      0|\n","| 100021|2010-03-01|   6.6212272644|           0.0078125|     44|        3| 85.51353836039999|   384.0|             2|                  0|                  1|    0.7142857142857143|      78.892311096|                0.0|       6.6212272644|     13.19863019662857|    1|      0|\n","| 100021|2010-04-01|   6.6212272644|0.009650180940892641|     38|        4|    135.6201362604|   414.5|             3|                  0|                  1|    0.8666666666666667|     128.998908996|                0.0|       6.6212272644|    20.918648783253335|    1|      0|\n","| 100021|2010-05-01|   6.6212272644|0.011235955056179775|     25|        5|162.66746520979999|   445.0|             4|                  0|                  1|                1.0625|    156.0462379454|                0.0|       6.6212272644|      29.3641231058875|    1|      0|\n","| 100021|2010-06-01|   6.6212272644|0.010515247108307046|     51|        5|162.66746520979999|   475.5|             4|                  1|                  1|    1.2352941176470589|    156.0462379454|      33.0805740352|       6.6212272644|    36.816012214094116|    1|      0|\n","| 100021|2010-07-01|   6.6212272644|0.009881422924901186|     82|        5|162.66746520979999|   506.0|             3|                  1|                  1|    1.3333333333333333|122.96566391019999|      33.0805740352|       6.6212272644|    41.602103974988886|    1|      0|\n","| 100021|2010-08-01|   6.6212272644|  0.0111731843575419|    109|        6|    213.5665588378|   537.0|             4|                  2|                  0|    1.4736842105263157|    173.8647575382|       78.892311096|                0.0|    48.563296267789475|    1|      0|\n","| 100021|2010-09-01|   6.6212272644|0.012334801762114538|     28|        7|228.85417938179998|   567.5|             4|                  3|                  0|                   1.6|    143.3406410214|      128.998908996|                0.0|    53.302163505470006|    1|      0|\n","+-------+----------+---------------+--------------------+-------+---------+------------------+--------+--------------+-------------------+-------------------+----------------------+------------------+-------------------+-------------------+----------------------+-----+-------+\n","only showing top 20 rows\n","\n"]}],"source":["# Printing a sample of the dataset\n","cltv_df.show()"]},{"cell_type":"markdown","metadata":{"id":"-5p6HDaA0YZJ"},"source":["## Storing the Processed Dataset as Parquet Files"]},{"cell_type":"code","execution_count":56,"metadata":{"executionInfo":{"elapsed":16263,"status":"ok","timestamp":1722047533423,"user":{"displayName":"Felipe Tufaile","userId":"10526460027904460804"},"user_tz":180},"id":"Nwa59wh9yHj-"},"outputs":[],"source":["# Defining dataframe path\n","file_path = \"../content/drive/MyDrive/Colab/Sandbox/customer_spend_model\"\n","\n","# Storing Customer Lifetime Value Data\n","cltv_df.write.format(\"parquet\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uzqhwoiTxkUE"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMWcgdiYIOUU7pYSlyIzwLW","machine_shape":"hm","mount_file_id":"1sDhmmlSQ-pm01hDJDr37sExWnq-Ysa8c","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
